{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "This exercise is about generating texts. For this purpose, the following steps have to be accomplished:\n",
    "* Load text corpus\n",
    "* Split corpus in sentences\n",
    "* Preprocess corpus\n",
    "* Count bi-grams from sentences\n",
    "* Compute bi-gram probabilies\n",
    "* Generate sentences from bi-grams\n",
    "* Generalise bi-gram Code to n-grams\n",
    "* Adjust Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fk169/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load corpus\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "sentences = nltk.corpus.gutenberg.sents(\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess corpus\n",
    "# casefold each word in file\n",
    "def casefold_file(file):\n",
    "    return [word.casefold() for word in file]\n",
    "import re\n",
    "# regular expression to remove all unnecessary characters\n",
    "characters_to_remove = \"[\\[\\]\\(\\){},.!\\?;:\\-_'\\\"\\&]\"\n",
    "\n",
    "# iterate the file, clean the words and remove empty strings\n",
    "def clean_file(file, removelist):\n",
    "    return [token for token in [re.sub(removelist,'',word) for word in file] if token != '']\n",
    "\n",
    "#sentences = [clean_file(casefold_file(s), characters_to_remove) for s in sentences]\n",
    "\n",
    "#sentences = [s for s in sentences if len(s) > 2]\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('said', 'the'), 209),\n",
       " (('START', 'i'), 168),\n",
       " (('of', 'the'), 133),\n",
       " (('START', 'the'), 118),\n",
       " (('said', 'alice'), 104),\n",
       " (('START', 'said'), 98),\n",
       " (('in', 'a'), 97),\n",
       " (('START', 'alice'), 85),\n",
       " (('and', 'the'), 82),\n",
       " (('in', 'the'), 79)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count bi_grams\n",
    "import collections\n",
    "\n",
    "def bigrams(sentence):\n",
    "    #def bigram(sentence):\n",
    "    bigrams = list(zip(sentence[:-1],sentence[1:]))\n",
    "    bigrams.append((\"START\",sentence[0]))\n",
    "    bigrams.append((sentence[len(sentence)-1],\"END\"))\n",
    "    return collections.Counter(bigrams)\n",
    "    \n",
    "bigram_sent = [bigrams(s) for s in sentences]\n",
    "counter = collections.Counter()\n",
    "for c in bigram_sent:\n",
    "    counter += c\n",
    "\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fk169/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# get number of elements\n",
    "import numpy as np\n",
    "unique = collections.Counter([k for k,_ in counter])\n",
    "names = list(unique)\n",
    "names.append(\"END\")\n",
    "\n",
    "\n",
    "# create matrix\n",
    "transitions = np.zeros((len(names), len(names)))\n",
    "transitions\n",
    "\n",
    "for k,v in counter:\n",
    "    try:\n",
    "        transitions[names.index(k),names.index(v)] = counter[(k,v)]\n",
    "    except ValueError:\n",
    "        print(k[0],k[1])\n",
    "\n",
    "# ignore potential errors  \n",
    "transitions /= transitions.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "alice\n",
      "as\n",
      "far\n",
      "out\n",
      "of\n",
      "me\n",
      "said\n",
      "the\n",
      "doorway\n",
      "and\n",
      "behind\n",
      "us\n",
      "drawling\n",
      "master\n",
      "says\n",
      "it\n",
      "s\n",
      "no\n",
      "time\n",
      "it\n",
      "was\n",
      "how\n",
      "is\n",
      "of\n",
      "course\n",
      "here\n",
      "alice\n",
      "began\n",
      "talking\n",
      "over\n",
      "to\n",
      "speak\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "word= \"START\"\n",
    "print(word)\n",
    "while word != \"END\":\n",
    "    wordI = np.argmax(np.random.multinomial(1, transitions[names.index(word),:], size=1))\n",
    "    \n",
    "    word = names[wordI]\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 's', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', '1865']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('alice', 's', 'adventures'),\n",
       " ('s', 'adventures', 'in'),\n",
       " ('adventures', 'in', 'wonderland'),\n",
       " ('in', 'wonderland', 'by'),\n",
       " ('wonderland', 'by', 'lewis'),\n",
       " ('by', 'lewis', 'carroll'),\n",
       " ('lewis', 'carroll', '1865')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentences[0]\n",
    "print(sentence)\n",
    "n=3\n",
    "\n",
    "list(zip(*[sentence[i:] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('*', '*', '*', '*'), 33),\n",
       " (('said', 'the', 'mock', 'turtle'), 19),\n",
       " (('she', 'said', 'to', 'herself'), 16),\n",
       " (('START', 'i', 'don', 't'), 14),\n",
       " (('said', 'the', 'caterpillar', 'END'), 12),\n",
       " (('a', 'minute', 'or', 'two'), 11),\n",
       " (('the', 'march', 'hare', 'END'), 10),\n",
       " (('you', 'won', 't', 'you'), 10),\n",
       " (('said', 'the', 'king', 'END'), 10),\n",
       " (('START', '*', '*', '*'), 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(sentence, n=2):\n",
    "    words = [\"START\", *sentence, \"END\"]\n",
    "    return collections.Counter(list(zip(*[words[i:] for i in range(n)])))\n",
    "\n",
    "ngram_sent = [ngrams(s,4) for s in sentences]\n",
    "counter = collections.Counter()\n",
    "for c in ngram_sent:\n",
    "    counter += c\n",
    "\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# get the starting states\n",
    "# get starts only\n",
    "starts = [(key, counter[key]) for _,key in enumerate(counter) if \"START\" in key] \n",
    "#print(starts)\n",
    "prior_names = list(zip(*starts))[0]\n",
    "prior_values = np.array(list(list(zip(*starts))[1]))\n",
    "prior_values = prior_values / np.sum(prior_values)\n",
    "\n",
    "#get evrything except for starts\n",
    "nstarts = [(key[:-1], key[-1], counter[key]) for _,key in enumerate(counter) if \"START\" not in key] \n",
    "#print(nstarts)\n",
    "# get the dimensions of the transition matrix\n",
    "state1 = list(set(list(zip(*nstarts))[0]))\n",
    "state2 = list(set(list(zip(*nstarts))[1]))\n",
    "\n",
    "t = np.zeros((len(state1),len(state2)))\n",
    "\n",
    "for s,e,v in nstarts:\n",
    "    t[state1.index(s),state2.index(e)]=v\n",
    "\n",
    "t /= t.sum(axis=1, keepdims=True)\n",
    "print(t.sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 's', 'the', 'queerest']\n",
      "['that', 's', 'the', 'queerest', 'thing']\n",
      "['that', 's', 'the', 'queerest', 'thing', 'about']\n",
      "['that', 's', 'the', 'queerest', 'thing', 'about', 'it']\n",
      "['that', 's', 'the', 'queerest', 'thing', 'about', 'it', 'END']\n"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "\n",
    "wordI = np.argmax(np.random.multinomial(1, prior_values, size=1))\n",
    "start_words = prior_names[wordI]\n",
    "\n",
    "\n",
    "last_words = start_words[1:]\n",
    "gen_text = list(last_words)\n",
    "while \"END\" not in gen_text:\n",
    "    idx = state1.index(tuple(last_words))\n",
    "    wordI = np.argmax(np.random.multinomial(1, t[idx,], size=1))\n",
    "    gen_text.append(state2[wordI])\n",
    "    print(gen_text)\n",
    "    last_words = gen_text[len(gen_text)-3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
